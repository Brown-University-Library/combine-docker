version: '3.2'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.1.1
    environment:
      - cluster.name=combine-elasticsearch
      - "ES_JAVA_OPTS=-Xms1024m -Xmx1024m"
      - xpack.security.enabled=false
      - http.max_initial_line_length=5m
    volumes:
      - esdata:/usr/share/elasticsearch/data
      - type: bind
        source: ./elasticsearch/elasticsearch.yml
        target: /usr/share/elasticsearch/config/elasticsearch.yml
        read_only: false # owner
    expose:
      - "9200"
    networks:
      combinenet:
        ipv4_address: 10.5.0.2


  mongo:
    build: ./mongo
    volumes:
      - mongodata:/data/db
    expose:
      - "27017"
    command: "--config /etc/mongod.conf"
    networks:
      combinenet:
        ipv4_address: 10.5.0.3


  mysql:
    build: ./mysql
    environment:
      MYSQL_ROOT_PASSWORD: combine
    volumes:
      - mysqldata:/var/lib/mysql
    # ports:
    #   - 3306:3306
    command: "--default-authentication-plugin=mysql_native_password"
    networks:
      combinenet:
        ipv4_address: 10.5.0.4


  redis:
    image: redis:4.0
    # ports:
    #   - 6379:6379
    networks:
      combinenet:
        ipv4_address: 10.5.0.5


  spark-cluster-base:
    build:
      context: ./spark_cluster_base
      dockerfile: Dockerfile
      args:
        SPARK_VERSION: "${SPARK_VERSION}"
        HADOOP_VERSION_SHORT: "${HADOOP_VERSION_SHORT}"
        ELASTICSEARCH_HADOOP_CONNECTOR_VERSION: "${ELASTICSEARCH_HADOOP_CONNECTOR_VERSION}"
    command:  /bin/true # exits code 0


  hadoop-namenode:
    build:
      context: ./hadoop
      dockerfile: Dockerfile
      args:
        HADOOP_VERSION: "${HADOOP_VERSION}"
    volumes:
      - type: volume
        source: hdfs
        target: /hdfs
        read_only: false
      - type: volume
        source: hadoop_binaries
        target: /opt/hadoop
        read_only: false # owner of dir
    expose:
      - "8020"
    # entrypoint: bash /tmp/entrypoint_namenode.sh
    command:  /opt/hadoop/bin/hdfs --config /opt/hadoop/etc/hadoop namenode
    networks:
      combinenet:
        ipv4_address: 10.5.0.6


  hadoop-datanode:
    build:
      context: ./hadoop
      dockerfile: Dockerfile
      args:
        HADOOP_VERSION: "${HADOOP_VERSION}"
    expose:
      - "50071"
      - "50075"
    command:  /opt/hadoop/bin/hdfs --config /opt/hadoop/etc/hadoop datanode
    networks:
      combinenet:
        ipv4_address: 10.5.0.7


  combine-django:
    build:
      context: .
      dockerfile: ./combine/Dockerfile
      args:
        LIVY_TAGGED_RELEASE: "${LIVY_TAGGED_RELEASE}"
        SCALA_VERSION: "${SCALA_VERSION}"
        COMBINE_BRANCH: "${COMBINE_BRANCH}"
    volumes:
      - ./combine/combine/static:/static
      - type: volume
        source: hdfs
        target: /hdfs
        read_only: true
      - type: volume
        source: hadoop_binaries
        target: /opt/hadoop
        read_only: true
      - type: volume
        source: spark_binaries
        target: /opt/spark
        read_only: true
      - type: volume
        source: livy_binaries
        target: /opt/livy
        read_only: true
      - type: bind # Bind Combine app submodule
        source: ./combine/combine
        target: /opt/combine
        read_only: false # owner
        bind:
          propagation: shared
      - type: volume
        source: combine_home
        target: /home/combine
        read_only: false # owner
      - type: volume
        source: combine_python_env
        target: /opt/conda/envs/combine
        read_only: false # owner
      - type: volume
        source: combine_tmp
        target: /tmp
        read_only: false
    expose:
      - "8000"
    command:  bash -c "nohup pyjxslt 6767 & > /tmp/pyjxslt.out && python /opt/combine/manage.py runserver 0.0.0.0:8000"
    depends_on:
      - mysql
    networks:
      combinenet:
        ipv4_address: 10.5.0.10


  combine-celery:
    image: combine-docker_combine-django
    volumes:
      - type: volume
        source: hdfs
        target: /hdfs
        read_only: true
      - type: volume
        source: hadoop_binaries
        target: /opt/hadoop
        read_only: true
      - type: volume
        source: spark_binaries
        target: /opt/spark
        read_only: true
      - type: volume
        source: livy_binaries
        target: /opt/livy
        read_only: true
      - type: bind # Bind Combine app submodule
        source: ./combine/combine
        target: /opt/combine
        read_only: false # owner
        bind:
          propagation: shared
      - type: volume
        source: combine_home
        target: /home/combine
        read_only: false # owner
      - type: volume
        source: combine_python_env
        target: /opt/conda/envs/combine
        read_only: false # owner
      - type: volume
        source: combine_tmp
        target: /tmp
        read_only: false
    working_dir: /opt/combine
    command:  celery -A core worker -l info --concurrency 1
    depends_on:
      - redis
    networks:
      combinenet:
        ipv4_address: 10.5.0.12


  livy:
    build:
      context: ./livy
      dockerfile: Dockerfile
      args:
        LIVY_TAGGED_RELEASE: "${LIVY_TAGGED_RELEASE}"
        SCALA_VERSION: "${SCALA_VERSION}"
    volumes:
      - type: volume
        source: hdfs
        target: /hdfs
        read_only: true
      - type: volume
        source: hadoop_binaries
        target: /opt/hadoop
        read_only: true
      - type: volume
        source: spark_binaries
        target: /opt/spark
        read_only: false # owner of /opt/spark when Livy only
      - type: volume
        source: livy_binaries
        target: /opt/livy
        read_only: false # owner of dir
      - type: bind # Bind Combine app submodule
        source: ./combine/combine
        target: /opt/combine
        read_only: false # owner
        bind:
          propagation: shared
      - type: volume
        source: combine_home
        target: /home/combine
        read_only: false
      - type: volume
        source: combine_python_env
        target: /opt/conda/envs/combine
        read_only: true
      - type: volume
        source: combine_tmp
        target: /tmp
        read_only: false
    expose:
      - "8998"
      - "4040"
    command:  bash -c "nohup pyjxslt 6767 & > /tmp/pyjxslt.out && /opt/livy/bin/livy-server"
    networks:
      combinenet:
        ipv4_address: 10.5.0.11

  nginx:
    image: nginx:latest
    container_name: nginx
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/error.log:/etc/nginx/error_log.log
      - ./nginx/cache/:/etc/nginx/cache
      - ./combine/combine/static:/static
    ports:
      - 80:80
      - 443:443
    depends_on:
      - combine-django
    networks:
      combinenet:
        ipv4_address: 10.5.0.13

volumes:

  # non-volatile
  esdata:
    driver: local
  mongodata:
    driver: local
  mysqldata:
    driver: local
  hdfs:
    driver: local
  combine_home:
    driver: local

  # volatile
  combine_python_env:
    driver: local
  hadoop_binaries:
    driver: local
  spark_binaries:
    driver: local
  livy_binaries:
    driver: local
  combine_tmp:
    driver: local


networks:
  combinenet:
    driver: bridge
    ipam:
     driver: default
     config:
       - subnet: 10.5.0.0/16
